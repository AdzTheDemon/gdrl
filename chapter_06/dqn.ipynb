{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, count\n",
    "from textwrap import wrap\n",
    "\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_monitored_env(env_name, monitor_mode, seed):\n",
    "    mdir = tempfile.mkdtemp()\n",
    "    env = gym.make(env_name)\n",
    "    env = wrappers.Monitor(env, mdir, force=True, mode=monitor_mode)\n",
    "    env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, log_scale=False):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    lines = [\"-\",\"--\",\":\",\"-.\"]\n",
    "    linecycler = cycle(lines)\n",
    "    for experiment, experiment_name in results:\n",
    "        label = '\\n'.join(wrap(experiment_name.replace('_', ', '), 50))\n",
    "        plt.plot(experiment, next(linecycler), label=label)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    if log_scale: plt.xscale('log')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_html(env_videos, title, max_n_videos=3):\n",
    "    videos = np.array(env_videos)\n",
    "    n_videos = min(max_n_videos, len(videos))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int)\n",
    "    videos = videos[idxs,:]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
    "        </video>\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gif_html(env_videos, title, max_n_videos=3):\n",
    "    videos = np.array(env_videos)\n",
    "    n_videos = min(max_n_videos, len(videos))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int)\n",
    "    videos = videos[idxs,:]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        basename = os.path.splitext(video_path)[0]\n",
    "        gif_path = basename + '.gif'\n",
    "        if not os.path.exists(gif_path):\n",
    "            ps = subprocess.Popen(\n",
    "                ('ffmpeg', \n",
    "                 '-i', video_path, \n",
    "                 '-r', '10', \n",
    "                 '-f', 'image2pipe', \n",
    "                 '-vcodec', 'ppm', \n",
    "                 '-'), \n",
    "                stdout=subprocess.PIPE)\n",
    "            output = subprocess.check_output(\n",
    "                ('convert', \n",
    "                 '-delay', '5', \n",
    "                 '-loop', '0', \n",
    "                 '-', gif_path), \n",
    "                stdin=ps.stdout)\n",
    "            ps.wait()\n",
    "\n",
    "        gif = io.open(gif_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(gif)\n",
    "            \n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <img src=\"data:image/gif;base64,{1}\" />\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQ(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu):\n",
    "        super(FCQ, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def load_experiences(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDuelingQ(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu):\n",
    "        super(FCDuelingQ, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "        self.output_value = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32).unsqueeze(0)      \n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        v = self.output_value(x)\n",
    "        a = self.output_layer(x)\n",
    "        q = v.expand_as(a) + (a - a.mean(1, keepdim=True).expand_as(a))\n",
    "        return q\n",
    "    \n",
    "    def load_experiences(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "\n",
    "    def __init__(self, max_samples=100000, batch_size=32):\n",
    "        self.memory = deque(maxlen=max_samples)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def store(self, sample):\n",
    "        self.memory.append(sample)\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        batch_size = self.batch_size if batch_size == None else batch_size\n",
    "        idxs = np.random.choice(len(self.memory), batch_size)\n",
    "        samples = np.array([self.memory[idx] for idx in idxs])\n",
    "        batches = [np.vstack(batch_type) for batch_type in samples.T]\n",
    "        return batches\n",
    "    \n",
    "    def is_prioritized(self):\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer():\n",
    "    def __init__(self, max_samples=100000, batch_size=32, rank_based=False, \n",
    "                 alpha=0.7, beta0=0.5, beta_rate=0.999965, epsilon=0.001):\n",
    "        self.memory = np.empty(shape=(max_samples, 3), dtype=np.ndarray)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_entries = 0\n",
    "        self.index_index = 0\n",
    "        self.td_error_index = 1\n",
    "        self.sample_index = 2\n",
    "        self.rank_based = rank_based # if not rank_based, then proportional\n",
    "        self.alpha = alpha # how much prioritization to use 0 is uniform (no priority), 1 is full priority\n",
    "        self.beta = beta0 # bias correction 0 is no correction 1 is full correction\n",
    "        self.beta0 = beta0 # beta0 is just beta's initial value\n",
    "        self.beta_rate = beta_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, idxs, td_errors):\n",
    "        self.memory[idxs, self.td_error_index] = np.abs(td_errors)\n",
    "\n",
    "    def store(self, sample):\n",
    "        self.memory[self.n_entries, self.index_index] = self.n_entries\n",
    "        priority = self.memory[:self.n_entries, self.td_error_index].max() if self.n_entries > 0 else 1.0\n",
    "        self.memory[self.n_entries, self.td_error_index] = priority\n",
    "        self.memory[self.n_entries, self.sample_index] = np.array(sample)\n",
    "        self.n_entries += 1\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        batch_size = self.batch_size if batch_size == None else batch_size\n",
    "        self.beta = min(1.0, self.beta * self.beta_rate**-1)\n",
    "        entries = self.memory[:self.n_entries]\n",
    "\n",
    "        if self.rank_based:\n",
    "            entries = entries[entries[:, self.td_error_index].argsort()[::-1]]\n",
    "            priorities = 1/(np.arange(self.n_entries) + 1)\n",
    "        else: # proportional\n",
    "            entries = entries[entries[:, self.td_error_index].argsort()[::-1]]\n",
    "            priorities = entries[:, self.td_error_index] + self.epsilon\n",
    "\n",
    "        scaled_priorities = priorities**self.alpha        \n",
    "        probs = np.array(scaled_priorities/np.sum(scaled_priorities), dtype=np.float64)\n",
    "\n",
    "        weights = (1.0/self.n_entries * 1.0/probs)**self.beta\n",
    "        normalized_weights = weights/weights.max()\n",
    "\n",
    "        idxs = np.random.choice(self.n_entries, batch_size, p=probs)\n",
    "        samples = np.array([entries[idx] for idx in idxs])\n",
    "        samples[:, self.td_error_index] = normalized_weights[idxs]\n",
    "        \n",
    "        samples_stacks = [np.vstack(batch_type) for batch_type in np.vstack(samples[:, self.sample_index]).T]\n",
    "        idxs_stack = np.vstack(samples[:, self.index_index])\n",
    "        weights_stack = np.vstack(samples[:, self.td_error_index])\n",
    "        return idxs_stack, weights_stack, samples_stacks\n",
    "\n",
    "    def is_prioritized(self):\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self):\n",
    "        self.exploratory_action_taken = False\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy()\n",
    "            return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyLinearStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, decay_rate=0.995, min_epsilon=0.01):\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        model.eval()\n",
    "\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.epsilon = max(self.min_epsilon, self.decay_rate*self.epsilon)\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyDecayStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, decay_rate=1e-5, min_epsilon=0.1):\n",
    "        self.t = 0\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.epsilon = init_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        model.eval()\n",
    "        \n",
    "        self.exploratory_action_taken = False\n",
    "        self.epsilon = max(self.init_epsilon * np.exp(-self.decay_rate * self.t), \n",
    "                           self.min_epsilon)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.t += 1\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaircaseStrategy():\n",
    "    def __init__(self, staircase):\n",
    "        self.t = 0\n",
    "        self.index = 0\n",
    "        self.epsilon = list(staircase.keys())[self.index]\n",
    "        self.staircase = staircase\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        model.eval()\n",
    "        \n",
    "        self.exploratory_action_taken = False\n",
    "        if list(self.staircase.values())[self.index] and self.t > list(self.staircase.values())[self.index]:\n",
    "            self.index += 1\n",
    "            self.epsilon = list(staircase.keys())[self.index]\n",
    "            self.t = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.t += 1\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxStrategy():\n",
    "    def __init__(self, init_temp=1.0, min_temp=0.5, exploration_ratio=0.9, max_steps=10000):\n",
    "        self.t = 0\n",
    "        self.init_temp = init_temp\n",
    "        self.exploration_ratio = exploration_ratio\n",
    "        self.min_temp = min_temp\n",
    "        self.max_steps = max_steps\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        model.eval()\n",
    "        \n",
    "        self.exploratory_action_taken = False\n",
    "        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)\n",
    "        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n",
    "        temp = np.clip(temp, self.min_temp, self.init_temp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            a = q_values - q_values.mean()\n",
    "            probs = np.exp(a/temp) / np.sum(np.exp(a/temp))\n",
    "            if sum(np.isnan(probs)):\n",
    "                isnan = np.isnan(probs)\n",
    "                probs[isnan] = 1.0/sum(isnan)\n",
    "            \n",
    "            assert np.isclose(probs.sum(), 1.0)\n",
    "            action = np.random.choice(np.arange(len(q_values)), size=1, p=probs)[0]    \n",
    "            self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, buffer, model, optimizer, strategy, \n",
    "                 update_target_every_timesteps, train_every_timesteps, \n",
    "                 double_learning_enabled, soft_update_enabled,\n",
    "                 tau, max_gradient, n_warmup_batch, \n",
    "                 print_every_episodes=25, save_every_episodes=500,\n",
    "                 experiment_name='default', random_seed=12345):\n",
    "\n",
    "        self.buffer = buffer()\n",
    "        self.model_fn = model\n",
    "        self.optimizer_fn = optimizer\n",
    "        self.strategy = strategy()\n",
    "        self.update_target_every_timesteps = update_target_every_timesteps\n",
    "        self.train_every_timesteps = train_every_timesteps\n",
    "        self.double_learning_enabled = double_learning_enabled\n",
    "        self.soft_update_enabled = soft_update_enabled\n",
    "        self.tau = tau\n",
    "        self.max_gradient = max_gradient\n",
    "        self.n_warmup_batch = n_warmup_batch\n",
    "        self.print_every_episodes = print_every_episodes\n",
    "        self.save_every_episodes = save_every_episodes\n",
    "        self.experiment_name = experiment_name\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        self.model.train()\n",
    "\n",
    "        if self.buffer.is_prioritized():\n",
    "            idxs, weights, (states, actions, rewards, next_states, is_terminals) = experiences\n",
    "        else:\n",
    "            states, actions, rewards, next_states, is_terminals = experiences\n",
    "            weights, idxs = np.ones_like(rewards), None\n",
    "        weights = torch.from_numpy(weights).float()\n",
    "        batch_size = len(is_terminals)\n",
    "        \n",
    "        if self.double_learning_enabled:\n",
    "            argmax_a_q_sp = self.model(next_states).detach().max(1)[1]\n",
    "        else:\n",
    "            argmax_a_q_sp = self.target(next_states).detach().max(1)[1]\n",
    "\n",
    "        q_sp = self.target(next_states).detach()\n",
    "        max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_s = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "        q_sa = self.model(states).gather(1, actions)\n",
    "\n",
    "        td_errors = q_sa - target_q_s\n",
    "        loss = (weights * td_errors).pow(2).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-self.max_gradient, \n",
    "                                   self.max_gradient)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.buffer.is_prioritized():\n",
    "            priorities = np.abs(td_errors.cpu().detach().numpy())\n",
    "            self.buffer.update(idxs, priorities)\n",
    "        \n",
    "        \n",
    "    def interaction_step(self, env, state):\n",
    "        action = self.strategy.select_action(self.model, state)\n",
    "        new_state, reward, is_terminal, _ = env.step(action)\n",
    "        experience = (state, action, reward, new_state, float(is_terminal))\n",
    "        self.buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += int(self.strategy.exploratory_action_taken)\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def train(self, env, gamma, max_episodes, max_mean_reward):\n",
    "        print('Starting experiment:\\n{}\\n'.format('\\n'.join(wrap(self.experiment_name.replace('_', ', '), 100))))\n",
    "        self.gamma = gamma\n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
    "        self.episode_exploration = []\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        \n",
    "        self.model = self.model_fn(nS, nA)\n",
    "        self.model.eval()\n",
    "        self.target = self.model_fn(nS, nA)\n",
    "        self.target.eval()\n",
    "        self.optimizer = self.optimizer_fn(self.model)\n",
    "        \n",
    "        for episode in range(1, max_episodes + 1):\n",
    "\n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for timestep in count(start=1):\n",
    "\n",
    "                state, is_terminal = self.interaction_step(env, state)\n",
    "                \n",
    "                if len(self.buffer) > self.buffer.batch_size * self.n_warmup_batch:\n",
    "                    \n",
    "                    experiences = self.buffer.sample()\n",
    "                    if self.buffer.is_prioritized():\n",
    "                        idxs, weights, samples = experiences\n",
    "                        experiences = self.model.load_experiences(samples)\n",
    "                        experiences = (idxs, weights) + (experiences,)\n",
    "                    else:\n",
    "                        experiences = self.model.load_experiences(experiences)\n",
    "                        \n",
    "                    if np.sum(self.episode_timestep) % self.train_every_timesteps == 0:\n",
    "                        self.optimize_model(experiences)\n",
    "                \n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_timesteps == 0:\n",
    "                    if self.soft_update_enabled:\n",
    "                        for target, network in zip(self.target.parameters(), \n",
    "                                                   self.model.parameters()):\n",
    "                            mixed_weights = (1.0 - self.tau) * target.data + self.tau * network.data\n",
    "                            target.data.copy_(mixed_weights)\n",
    "                    else:\n",
    "                        self.target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            mean_reward = np.mean(self.episode_reward[-100:])\n",
    "            if episode % self.print_every_episodes == 0 or mean_reward >= max_mean_reward:\n",
    "                episode_exploration_ratio = self.episode_exploration[-1] / self.episode_timestep[-1]\n",
    "                debug_message = 'ep {:04}, step {:06}, exploration rat {:.2f}, buff size {:06}, '\n",
    "                debug_message += 'last ep rew {:03}, last 100 ep mean rew {:.2f}'\n",
    "                debug_message = debug_message.format(\n",
    "                    episode, self.episode_timestep[-1], episode_exploration_ratio, \n",
    "                    len(self.buffer), int(self.episode_reward[-1]), mean_reward)\n",
    "                print(debug_message)\n",
    "\n",
    "            if episode % self.save_every_episodes == 0 or mean_reward >= max_mean_reward:\n",
    "                directory_name =  os.path.join('checkpoints', experiment_name.replace('_', '/'))\n",
    "                filename = '{}_{}_{}.ckp'.format(episode, timestep, mean_reward)\n",
    "                file_path =  os.path.join(directory_name, filename)\n",
    "                if not os.path.exists(directory_name):\n",
    "                    os.makedirs(directory_name)\n",
    "                torch.save(self.model.state_dict(), file_path)\n",
    "\n",
    "            if mean_reward >= max_mean_reward:\n",
    "                print('Training complete')\n",
    "                break\n",
    "\n",
    "        env.close()\n",
    "        print()\n",
    "        return self.episode_reward\n",
    "\n",
    "    \n",
    "    def evaluate(self, env, strategy, episodes=10, render=True):\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            state, is_terminal = env.reset(), False\n",
    "            rewards.append(0)\n",
    "            for t in count(start=1):\n",
    "                if render: env.render()\n",
    "                action = strategy.select_action(self.model, state)\n",
    "                state, reward, is_terminal, _ = env.step(action)\n",
    "                rewards[-1] += reward\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "        env.close()\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "buffers = {\n",
    "    'uniform': lambda: ReplayBuffer(\n",
    "        max_samples=100000, batch_size=32),\n",
    "    'prioritized': lambda: PrioritizedReplayBuffer(\n",
    "        max_samples=100000, batch_size=32, rank_based=True, \n",
    "        alpha=0.7, beta0=0.5, beta_rate=0.999965, epsilon=0.001)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'vanilla': lambda nS, nA: FCQ(\n",
    "        nS, nA, hidden_dims=(64,64)),\n",
    "    'dueling' : lambda nS, nA: FCDuelingQ(\n",
    "        nS, nA, hidden_dims=(64,64)),\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'adam': lambda net: optim.Adam(\n",
    "        net.parameters(), lr=0.0005), \n",
    "    'rmsprop': lambda net: optim.RMSprop(\n",
    "        net.parameters(), lr=0.0005)\n",
    "}\n",
    "\n",
    "strategies = {\n",
    "    'egreedylinear': lambda: EGreedyLinearStrategy(\n",
    "        init_epsilon=1.0, decay_rate=0.998, min_epsilon=0.1),\n",
    "    'egreedyexponential': lambda: EGreedyLinearStrategy(\n",
    "        init_epsilon=1.0, decay_rate=0.998, min_epsilon=0.1),\n",
    "    'staircase': lambda: StaircaseStrategy(\n",
    "        staircase={1.0:500, 0.25:1000, 0.5:1000, 0.25:5000, 0.1:None}),\n",
    "    'softmax': lambda: SoftMaxStrategy(\n",
    "        init_temp=1.0, min_temp=0.01, exploration_ratio=0.9, max_steps=10000)\n",
    "}\n",
    "\n",
    "update_target_every_timestepss = [5, 10, 20, 100]\n",
    "train_every_timestepss = [2, 4, 8]\n",
    "enable_double_learning = [True, False]\n",
    "enable_soft_updates = [True]\n",
    "taus = [0.01, 0.05, 0.10, 1.0]\n",
    "max_gradients = [1, 2, 4, 8]\n",
    "n_warmup_batches = [5, 10, 50]\n",
    "random_seeds = [12, 34, 56, 78, 90]\n",
    "gamma, max_episodes, max_mean_reward = 1.0, 1000, 200\n",
    "print_every_episodes, save_every_episodes = 150, 500\n",
    "\n",
    "training_results, evaluation_results = [], []\n",
    "BestEnv = namedtuple('best_env', ['env', 'mean_reward', 'num_episodes'])\n",
    "best_training_env = BestEnv(env=None, mean_reward=float('-inf'), num_episodes=float('inf'))\n",
    "best_evaluation_env = BestEnv(env=None, mean_reward=float('-inf'), num_episodes=float('inf'))\n",
    "\n",
    "for buffer in buffers:\n",
    "    for model in models:\n",
    "        for optimizer in optimizers:    \n",
    "            for strategy in strategies:\n",
    "                for update_target_every_timesteps in update_target_every_timestepss:\n",
    "                    for train_every_timesteps in train_every_timestepss:\n",
    "                        for double_learning_enabled in enable_double_learning:\n",
    "                            for soft_update_enabled in enable_soft_updates:\n",
    "                                for tau in taus:\n",
    "                                    for max_gradient in max_gradients:\n",
    "                                        for n_warmup_batch in n_warmup_batches:\n",
    "                                            experiment_name = ''\n",
    "                                            experiment_name += '{}_{}_{}_{}'\n",
    "                                            experiment_name += '_update-target-every-timesteps-{}'\n",
    "                                            experiment_name += '_train-every-timesteps-{}'\n",
    "                                            experiment_name += '_double-learning-enabled-{}'\n",
    "                                            experiment_name += '_soft-update-enabled-{}'\n",
    "                                            experiment_name += '_tau-{}'\n",
    "                                            experiment_name += '_max-gradient-{}'\n",
    "                                            experiment_name += '_n-warmup-batch-{}'\n",
    "\n",
    "                                            experiment_name = experiment_name.format(\n",
    "                                                buffer, model, optimizer,  strategy, \n",
    "                                                update_target_every_timesteps, train_every_timesteps,\n",
    "                                                double_learning_enabled, soft_update_enabled, tau,\n",
    "                                                max_gradient, n_warmup_batch\n",
    "                                            )\n",
    "                                            \n",
    "                                            training_rewards, evaluation_rewards = [], []\n",
    "                                            for seed in random_seeds:\n",
    "                                                print('Using seed {}'.format(seed))\n",
    "                                                torch.manual_seed(seed)\n",
    "                                                np.random.seed(seed)\n",
    "\n",
    "                                                agent = DQNAgent(\n",
    "                                                    buffers[buffer], models[model], optimizers[optimizer], \n",
    "                                                    strategies[strategy], \n",
    "                                                    update_target_every_timesteps, train_every_timesteps,\n",
    "                                                    double_learning_enabled, soft_update_enabled, tau,\n",
    "                                                    max_gradient, n_warmup_batch, \n",
    "                                                    print_every_episodes=print_every_episodes,\n",
    "                                                    save_every_episodes=save_every_episodes, \n",
    "                                                    experiment_name=experiment_name)\n",
    "\n",
    "                                                training_env = make_monitored_env(env_name, 'training', seed)\n",
    "                                                rewards = agent.train(\n",
    "                                                    training_env, gamma=gamma, \n",
    "                                                    max_episodes=max_episodes, max_mean_reward=max_mean_reward)\n",
    "                                                training_rewards.append(rewards)\n",
    "                                                mean_reward = np.mean(rewards)\n",
    "                                                num_episodes = len(rewards)\n",
    "                                                if mean_reward > best_training_env.mean_reward or \\\n",
    "                                                   (mean_reward == best_training_env.mean_reward and \\\n",
    "                                                    num_episodes < best_training_env.num_episodes):\n",
    "                                                    best_training_env = BestEnv(\n",
    "                                                        env=training_env, \n",
    "                                                        mean_reward=mean_reward, \n",
    "                                                        num_episodes=num_episodes)\n",
    "                                                \n",
    "                                                evaluation_env = make_monitored_env(env_name, 'evaluation', seed)\n",
    "                                                rewards = agent.evaluate(\n",
    "                                                    evaluation_env, GreedyStrategy(), episodes=10, render=False)\n",
    "                                                evaluation_rewards.append(rewards)\n",
    "                                                mean_reward = np.mean(rewards)\n",
    "                                                num_episodes = len(rewards)\n",
    "                                                if mean_reward > best_evaluation_env.mean_reward or \\\n",
    "                                                   (mean_reward == best_evaluation_env.mean_reward and \\\n",
    "                                                    num_episodes < best_evaluation_env.num_episodes):\n",
    "                                                    best_evaluation_env = BestEnv(\n",
    "                                                        env=evaluation_env, \n",
    "                                                        mean_reward=mean_reward, \n",
    "                                                        num_episodes=num_episodes)\n",
    "\n",
    "                                            output = np.empty((len(training_rewards), max_episodes,))\n",
    "                                            output[:] = float('-inf')\n",
    "                                            for i, training_reward in enumerate(training_rewards):\n",
    "                                                output[i, :len(training_reward)] = training_reward\n",
    "                                            output = np.mean(output, axis=0)\n",
    "                                            output = output[output>=0]\n",
    "                                            training_results.append((output, experiment_name))\n",
    "                                            \n",
    "                                            output = np.empty((len(evaluation_rewards), max_episodes,))\n",
    "                                            output[:] = float('-inf')\n",
    "                                            for i, evaluation_reward in enumerate(evaluation_rewards):\n",
    "                                                output[i, :len(evaluation_reward)] = evaluation_reward\n",
    "                                            output = np.mean(output, axis=0)\n",
    "                                            output = output[output>=0]\n",
    "                                            evaluation_results.append((output, experiment_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(training_results, log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(training_results, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(evaluation_results, log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(evaluation_results, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=best_training_env.env.videos, title='Training evolution', max_n_videos=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_evaluation_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=best_evaluation_env.env.videos, title='Evaluation results', max_n_videos=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
