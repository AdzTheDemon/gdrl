{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQ(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu, seed=123):\n",
    "        super(FCQ, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def load_experiences(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDuelingQ(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=(32,32), activation_fc=F.relu, seed=123):\n",
    "        super(FCDuelingQ, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "        self.output_value = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32).unsqueeze(0)      \n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        v = self.output_value(x)\n",
    "        a = self.output_layer(x)\n",
    "        q = v.expand_as(a) + (a - a.mean(1, keepdim=True).expand_as(a))\n",
    "        return q\n",
    "    \n",
    "    def load_experiences(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_samples=100000, batch_size=64):\n",
    "        self.memory = deque(maxlen=max_samples)\n",
    "        self.batch_size = batch_size\n",
    "    def store(self, sample):\n",
    "        self.memory.append(sample)\n",
    "    def sample(self, batch_size=None):\n",
    "        batch_size = self.batch_size if batch_size == None else batch_size\n",
    "        idxs = np.random.choice(len(self.memory), batch_size)\n",
    "        samples = np.array([self.memory[idx] for idx in idxs])\n",
    "        batches = [np.vstack(batch_type) for batch_type in samples.T]\n",
    "        return batches\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def select_action(self, q_network, state):\n",
    "        q_network.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state).cpu().detach().data.numpy()\n",
    "            return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyLinearStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, decay_rate=0.995, min_epsilon=0.01):\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def select_action(self, q_network, state):\n",
    "        q_network.eval()\n",
    "\n",
    "        nA = q_network.output_layer.out_features\n",
    "        action = np.random.randint(nA)\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_values = q_network(state).cpu().detach().data.numpy()\n",
    "                action = np.argmax(q_values)\n",
    "        self.epsilon = max(self.min_epsilon, self.decay_rate*self.epsilon)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyDecayStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, decay_rate=1e-4, min_epsilon=0.1):\n",
    "        self.t = 0\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def select_action(self, q_network, state):\n",
    "        q_network.eval()\n",
    "        \n",
    "        nA = q_network.output_layer.out_features\n",
    "        epsilon = max(self.init_epsilon * np.exp(-self.decay_rate * self.t), \n",
    "                      self.min_epsilon)\n",
    "        #if self.t % 100 == 0:\n",
    "        # print(epsilon)\n",
    "        action = np.random.randint(nA)\n",
    "        if np.random.rand() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_values = q_network(state).cpu().detach().data.numpy()\n",
    "                action = np.argmax(q_values)\n",
    "        self.t += 1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxStrategy():\n",
    "    def __init__(self, init_temp=1.0, exploration_ratio=0.9, min_temp=0.5, max_episodes=1000):\n",
    "        self.t = 0\n",
    "        self.init_temp = init_temp\n",
    "        self.exploration_ratio = exploration_ratio\n",
    "        self.min_temp = min_temp\n",
    "        self.max_episodes = max_episodes\n",
    "\n",
    "    def select_action(self, q_network, state):\n",
    "        q_network.eval()\n",
    "        nA = q_network.output_layer.out_features\n",
    "        nS = q_network.input_layer.in_features\n",
    "\n",
    "        temp = 1 - self.t / (self.max_episodes * self.exploration_ratio)\n",
    "        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n",
    "        temp = np.clip(temp, self.min_temp, self.init_temp)\n",
    "        temp = np.round(max(temp, 0.01), 2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q = q_network(state).cpu().detach().numpy()\n",
    "            a = q - q.mean()\n",
    "            a_scaled = a/temp\n",
    "            exp_a = np.exp(a/temp)\n",
    "            probs = exp_a / np.sum(exp_a)\n",
    "            isnan = np.isnan(probs)\n",
    "            probs[isnan] = 1.0/sum(isnan)\n",
    "            assert np.isclose(probs.sum(), 1.0)\n",
    "            action = np.random.choice(np.arange(nA), size=1, p=probs)[0]\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, replay_buffer, q_network, q_target, optimizer, \n",
    "                 behavioral_strategy, update_every_timesteps=10, polyak_avg_update=True, double_learning=True,\n",
    "                 gamma=1.0, tau=0.1, debug_print_every_episodes=25, save_model_every_episodes=500):\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.q_network = q_network\n",
    "        self.q_target = q_target\n",
    "        self.optimizer = optimizer\n",
    "        self.behavioral_strategy = behavioral_strategy\n",
    "        self.update_every_timesteps = update_every_timesteps\n",
    "        self.polyak_avg_update = polyak_avg_update\n",
    "        self.double_learning = double_learning\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.debug_print_every_episodes = debug_print_every_episodes\n",
    "        self.save_model_every_episodes = save_model_every_episodes\n",
    "\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        self.q_network.train()\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        if self.double_learning:\n",
    "            argmax_a_q_sp = self.q_network(next_states).detach().max(1)[1]\n",
    "        else:\n",
    "            argmax_a_q_sp = self.q_target(next_states).detach().max(1)[1]\n",
    "\n",
    "        max_a_q_sp = self.q_target(next_states).detach()\n",
    "        max_a_q_sp = max_a_q_sp[np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_s = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
    "\n",
    "        q_sa = self.q_network(states).gather(1, actions)\n",
    "        \n",
    "        loss = F.mse_loss(q_sa, target_q_s)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.q_network.parameters():\n",
    "            param.grad.data.clamp_(-10, 10)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def train(self, env, max_episodes, max_mean_reward):\n",
    "\n",
    "        nS = env.observation_space.shape[0]\n",
    "        nA = env.action_space.n\n",
    "        total_timestep = 0\n",
    "        episode_reward = []\n",
    "        self.q_target.eval()\n",
    "        \n",
    "        for episode in range(max_episodes+1):\n",
    "\n",
    "            state, done = env.reset(), False\n",
    "            episode_reward.append(0)\n",
    "\n",
    "            for timestep in itertools.count():\n",
    "                total_timestep += 1\n",
    "                \n",
    "                action = self.behavioral_strategy.select_action(self.q_network, state)\n",
    "                new_state, reward, is_terminal, _ = env.step(action)\n",
    "                episode_reward[-1] += reward\n",
    "\n",
    "                experience = (state, action, reward, new_state, float(is_terminal))\n",
    "                self.replay_buffer.store(experience)\n",
    "\n",
    "                state = new_state\n",
    "                \n",
    "                if len(self.replay_buffer) > self.replay_buffer.batch_size * 10:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.q_network.load_experiences(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "                \n",
    "                if total_timestep % self.update_every_timesteps == 0:\n",
    "                    if self.polyak_avg_update:\n",
    "                        for target_param, local_param in zip(self.q_target.parameters(), self.q_network.parameters()):\n",
    "                            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "                    else:\n",
    "                        self.q_target.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "                if is_terminal:\n",
    "                    break\n",
    "\n",
    "            mean_reward = np.mean(episode_reward[-100:])\n",
    "            if episode % self.debug_print_every_episodes == 0:\n",
    "                print('ep {:04}, step {:06} last ep rew {:03} last 100 ep mean rew {:.2f}'.format(\n",
    "                    episode, total_timestep, int(episode_reward[-1]), mean_reward))\n",
    "            if episode % self.save_model_every_episodes == 0:\n",
    "                torch.save(agent.q_network.state_dict(), 'checkpoints/checkpoint_{}.pth'.format(episode))\n",
    "            if mean_reward >= max_mean_reward:\n",
    "                print('ep {:04}, step {:06} last ep rew {:03} last 100 ep mean rew {:.2f}'.format(\n",
    "                    episode, total_timestep, int(episode_reward[-1]), mean_reward))\n",
    "                torch.save(agent.q_network.state_dict(), 'checkpoints/checkpoint_{}.pth'.format(episode))\n",
    "                break\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0000, step 000024 last ep rew 024 last 100 ep mean rew 24.00\n",
      "ep 0025, step 000367 last ep rew 011 last 100 ep mean rew 14.12\n",
      "ep 0050, step 000630 last ep rew 015 last 100 ep mean rew 12.35\n",
      "ep 0075, step 000890 last ep rew 010 last 100 ep mean rew 11.71\n",
      "ep 0100, step 001432 last ep rew 027 last 100 ep mean rew 14.08\n",
      "ep 0125, step 002827 last ep rew 097 last 100 ep mean rew 24.60\n",
      "ep 0150, step 006884 last ep rew 167 last 100 ep mean rew 62.54\n",
      "ep 0175, step 010909 last ep rew 134 last 100 ep mean rew 100.19\n",
      "ep 0200, step 014798 last ep rew 159 last 100 ep mean rew 133.66\n",
      "ep 0225, step 018843 last ep rew 158 last 100 ep mean rew 160.16\n",
      "ep 0250, step 023276 last ep rew 174 last 100 ep mean rew 163.92\n",
      "ep 0275, step 028055 last ep rew 178 last 100 ep mean rew 171.46\n"
     ]
    }
   ],
   "source": [
    "mdir = tempfile.mkdtemp()\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, mdir, force=True, mode='training')\n",
    "env.seed(1234)\n",
    "nS = env.observation_space.shape[0]\n",
    "nA = env.action_space.n\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_samples=1000000, batch_size=64)\n",
    "q_network = FCDuelingQ(nS, nA, hidden_dims=(64,64))\n",
    "q_target = FCDuelingQ(nS, nA, hidden_dims=(64,64))\n",
    "optimizer = optim.RMSprop(q_network.parameters(), lr=0.0005)\n",
    "behavioral_strategy = EGreedyLinearStrategy()\n",
    "\n",
    "agent = DQNAgent(replay_buffer, \n",
    "                 q_network, \n",
    "                 q_target, \n",
    "                 optimizer, \n",
    "                 behavioral_strategy)\n",
    "\n",
    "mdir = tempfile.mkdtemp()\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, mdir, force=True, mode='training')\n",
    "agent.train(env, max_episodes=1000, max_mean_reward=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/tmp/tmp73_4v8to/openaigym.video.1.20908.video000000.mp4',\n",
       "  '/tmp/tmp73_4v8to/openaigym.video.1.20908.video000000.meta.json'),\n",
       " ('/tmp/tmp73_4v8to/openaigym.video.1.20908.video000001.mp4',\n",
       "  '/tmp/tmp73_4v8to/openaigym.video.1.20908.video000001.meta.json'),\n",
       " ('/tmp/tmp73_4v8to/openaigym.video.1.20908.video000008.mp4',\n",
       "  '/tmp/tmp73_4v8to/openaigym.video.1.20908.video000008.meta.json'),\n",
       " ('/tmp/tmp73_4v8to/openaigym.video.1.20908.video000027.mp4',\n",
       "  '/tmp/tmp73_4v8to/openaigym.video.1.20908.video000027.meta.json'),\n",
       " ('/tmp/tmp73_4v8to/openaigym.video.1.20908.video000064.mp4',\n",
       "  '/tmp/tmp73_4v8to/openaigym.video.1.20908.video000064.meta.json'),\n",
       " ('/tmp/tmp73_4v8to/openaigym.video.1.20908.video000125.mp4',\n",
       "  '/tmp/tmp73_4v8to/openaigym.video.1.20908.video000125.meta.json')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_videos_html(env_videos, max_n_videos=3):\n",
    "    videos = np.array(env_videos=videos)\n",
    "    n_videos = min(max_n_videos, len(videos))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int)\n",
    "    videos = videos[idxs,:]\n",
    "\n",
    "    strm = ''\n",
    "    for video_path, meta_path in videos:\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h2>{0}<h2/>\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
    "        </video>\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(data=get_videos_html(env.videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gif_html(env_videos, max_n_videos=3):\n",
    "    videos = np.array(env_videos)\n",
    "    n_videos = min(max_n_videos, len(videos))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int)\n",
    "    videos = videos[idxs,:]\n",
    "\n",
    "    strm = ''\n",
    "    for video_path, meta_path in videos:\n",
    "        basename = os.path.splitext(video_path)[0]\n",
    "        gif_path = basename + '.gif'\n",
    "        if not os.path.exists(gif_path):\n",
    "            ps = subprocess.Popen(\n",
    "                ('ffmpeg', \n",
    "                 '-i', video_path, \n",
    "                 '-r', '10', \n",
    "                 '-f', 'image2pipe', \n",
    "                 '-vcodec', 'ppm', \n",
    "                 '-'), \n",
    "                stdout=subprocess.PIPE)\n",
    "            output = subprocess.check_output(\n",
    "                ('convert', \n",
    "                 '-delay', '5', \n",
    "                 '-loop', '0', \n",
    "                 '-', gif_path), \n",
    "                stdin=ps.stdout)\n",
    "            ps.wait()\n",
    "\n",
    "        gif = io.open(gif_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(gif)\n",
    "            \n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h2>{0}<h2/>\n",
    "        <img src=\"data:image/gif;base64,{1}\" />\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env.videos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
