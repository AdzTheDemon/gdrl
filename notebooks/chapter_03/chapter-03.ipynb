{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PI, VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "\n",
    "import gym, gym_walk, gym_aima\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "import random\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "random.seed(123); np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(pi, P, action_symbols=('<', 'v', '>', '^'), n_cols=4, title='Policy:'):\n",
    "    print(title)\n",
    "    arrs = {k:v for k,v in enumerate(action_symbols)}\n",
    "    for s in range(len(P)):\n",
    "        a = pi(s)\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_function(V, P, n_cols=4, title='State-value function:'):\n",
    "    print(title)\n",
    "    for s in range(len(P)):\n",
    "        v = V[s]\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), '{:.2f}'.format(v).rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_success(env, pi, goal_state, iterations=1000):\n",
    "    random.seed(123); np.random.seed(123) ; env.seed(123)\n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            state, _, done, _ = env.step(pi(state))\n",
    "        results.append(state == goal_state)\n",
    "    return np.sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_return(env, pi, iterations=1000):\n",
    "    random.seed(123); np.random.seed(123) ; env.seed(123)\n",
    "    results = []\n",
    "    for i in range(iterations):\n",
    "        state, done = env.reset(), False\n",
    "        results.append(0.0)\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(pi(state))\n",
    "            results[-1] += reward\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slippery Walk Five MDP and sample policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "|           | 01      < | 02      < | 03      < | 04      < | 05      < |           |\n",
      "Reaches goal 4.90%. Obtains an average undiscounted return of 0.0490.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SlipperyWalkFive-v0')\n",
    "P = env.env.P\n",
    "\n",
    "WEST, EAST = range(2)\n",
    "pi = lambda s: {\n",
    "    0:WEST, 1:WEST, 2:WEST, 3:WEST, 4:WEST, 5:WEST, 6:WEST\n",
    "}[s]\n",
    "print_policy(pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi, goal_state=6)*100, \n",
    "    mean_return(env, pi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n",
    "    prev_V = np.zeros(len(P))\n",
    "    while True:\n",
    "        V = np.zeros(len(P))\n",
    "        for s in range(len(P)):\n",
    "            for prob, next_state, reward, done in P[s][pi(s)]:\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "        if np.max(np.abs(prev_V - V)) < theta:\n",
    "            break\n",
    "        prev_V = V.copy()\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "|           | 01   0.00 | 02   0.01 | 03   0.04 | 04   0.11 | 05   0.33 |           |\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(pi, P)\n",
    "print_state_value_function(V, P, n_cols=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, P, gamma=1.0):\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "    for s in range(len(P)):\n",
    "        for a in range(len(P[s])):\n",
    "            for prob, next_state, reward, done in P[s][a]:\n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "|           | 01      > | 02      > | 03      > | 04      > | 05      > |           |\n",
      "Reaches goal 95.10%. Obtains an average undiscounted return of 0.9510.\n"
     ]
    }
   ],
   "source": [
    "improved_pi = policy_improvement(V, P)\n",
    "print_policy(improved_pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, improved_pi, goal_state=6)*100, \n",
    "    mean_return(env, improved_pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "|           | 01   0.67 | 02   0.89 | 03   0.96 | 04   0.99 | 05   1.00 |           |\n"
     ]
    }
   ],
   "source": [
    "# how about we evaluate the improved policy?\n",
    "improved_V = policy_evaluation(improved_pi, P)\n",
    "print_state_value_function(improved_V, P, n_cols=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "|           | 01      > | 02      > | 03      > | 04      > | 05      > |           |\n",
      "Reaches goal 95.10%. Obtains an average undiscounted return of 0.9510.\n"
     ]
    }
   ],
   "source": [
    "# can we improved the improved policy?\n",
    "improved_improved_pi = policy_improvement(improved_V, P)\n",
    "print_policy(improved_improved_pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, improved_improved_pi, goal_state=6)*100, \n",
    "    mean_return(env, improved_improved_pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "|           | 01   0.67 | 02   0.89 | 03   0.96 | 04   0.99 | 05   1.00 |           |\n"
     ]
    }
   ],
   "source": [
    "# it is the same policy\n",
    "# if we evaluate again, we can see there is nothing to improve \n",
    "# that also means we reached the optimal policy\n",
    "improved_improved_V = policy_evaluation(improved_improved_pi, P)\n",
    "print_state_value_function(improved_improved_V, P, n_cols=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state-value function didn't improve, then we reach the optimal policy\n",
    "np.all(improved_V == improved_improved_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, gamma=1.0):\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))\n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]\n",
    "    while True:\n",
    "        old_pi = {s:pi(s) for s in range(len(P))}\n",
    "        V = policy_evaluation(pi, P, gamma)\n",
    "        pi = policy_improvement(V, P, gamma)\n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}:\n",
    "            break\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "|           | 01      > | 02      > | 03      > | 04      > | 05      > |           |\n",
      "Reaches goal 95.10%. Obtains an average undiscounted return of 0.9510.\n",
      "\n",
      "State-value function:\n",
      "|           | 01   0.67 | 02   0.89 | 03   0.96 | 04   0.99 | 05   1.00 |           |\n"
     ]
    }
   ],
   "source": [
    "optimal_V, optimal_pi = policy_iteration(P)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(optimal_pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, optimal_pi, goal_state=6)*100, \n",
    "    mean_return(env, optimal_pi)))\n",
    "print()\n",
    "print_state_value_function(optimal_V, P, n_cols=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(improved_V == optimal_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake MDP and sample policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "| 00      > | 01      < | 02      v | 03      ^ |\n",
      "| 04      < |           | 06      > |           |\n",
      "| 08      ^ | 09      v | 10      ^ |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 11.50%. Obtains an average undiscounted return of 0.1150.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "P = env.env.P\n",
    "\n",
    "WEST, SOUTH, EAST, NORTH = range(4)\n",
    "pi = lambda s: {\n",
    "    0:EAST, 1:WEST, 2:SOUTH, 3:NORTH,\n",
    "    4:WEST, 5:WEST, 6:EAST, 7:WEST,\n",
    "    8:NORTH, 9:SOUTH, 10:NORTH, 11:WEST,\n",
    "    12:WEST, 13:EAST, 14:SOUTH, 15:WEST\n",
    "}[s]\n",
    "print_policy(pi, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi, goal_state=15)*100, \n",
    "    mean_return(env, pi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "| 00   0.10 | 01   0.05 | 02   0.05 | 03   0.05 |\n",
      "| 04   0.15 |           | 06   0.05 |           |\n",
      "| 08   0.20 | 09   0.26 | 10   0.10 |           |\n",
      "|           | 13   0.50 | 14   0.74 |           |\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(pi, P, gamma=0.99)\n",
    "print_state_value_function(V, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "| 00      < | 01      ^ | 02      < | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 72.00%. Obtains an average undiscounted return of 0.7200.\n"
     ]
    }
   ],
   "source": [
    "new_pi = policy_improvement(V, P, gamma=0.99)\n",
    "print_policy(new_pi, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, new_pi, goal_state=15)*100, \n",
    "    mean_return(env, new_pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function:\n",
      "| 00   0.53 | 01   0.45 | 02   0.38 | 03   0.37 |\n",
      "| 04   0.55 |           | 06   0.32 |           |\n",
      "| 08   0.58 | 09   0.63 | 10   0.60 |           |\n",
      "|           | 13   0.73 | 14   0.86 |           |\n"
     ]
    }
   ],
   "source": [
    "new_V = policy_evaluation(new_pi, P, gamma=0.99)\n",
    "print_state_value_function(new_V, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "| 00      < | 01      ^ | 02      ^ | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 73.20%. Obtains an average undiscounted return of 0.7320.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.54 | 01   0.50 | 02   0.47 | 03   0.46 |\n",
      "| 04   0.56 |           | 06   0.36 |           |\n",
      "| 08   0.59 | 09   0.64 | 10   0.62 |           |\n",
      "|           | 13   0.74 | 14   0.86 |           |\n"
     ]
    }
   ],
   "source": [
    "V_best_p, pi_best_p = policy_iteration(P, gamma=0.99)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(pi_best_p, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best_p, goal_state=15)*100, \n",
    "    mean_return(env, pi_best_p)))\n",
    "print()\n",
    "print_state_value_function(V_best_p, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For comparison, original policy and state-value function:\n",
      "Policy:\n",
      "| 00      > | 01      < | 02      v | 03      ^ |\n",
      "| 04      < |           | 06      > |           |\n",
      "| 08      ^ | 09      v | 10      ^ |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 11.50%. Obtains an average undiscounted return of 0.1150.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.10 | 01   0.05 | 02   0.05 | 03   0.05 |\n",
      "| 04   0.15 |           | 06   0.05 |           |\n",
      "| 08   0.20 | 09   0.26 | 10   0.10 |           |\n",
      "|           | 13   0.50 | 14   0.74 |           |\n"
     ]
    }
   ],
   "source": [
    "print('For comparison, original policy and state-value function:')\n",
    "print_policy(pi, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi, goal_state=15)*100, \n",
    "    mean_return(env, pi)))\n",
    "print()\n",
    "print_state_value_function(V, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For comparison, improved policy and state-value function:\n",
      "Policy:\n",
      "| 00      < | 01      ^ | 02      < | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 72.00%. Obtains an average undiscounted return of 0.7200.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.53 | 01   0.45 | 02   0.38 | 03   0.37 |\n",
      "| 04   0.55 |           | 06   0.32 |           |\n",
      "| 08   0.58 | 09   0.63 | 10   0.60 |           |\n",
      "|           | 13   0.73 | 14   0.86 |           |\n"
     ]
    }
   ],
   "source": [
    "print('For comparison, improved policy and state-value function:')\n",
    "print_policy(new_pi, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, new_pi, goal_state=15)*100, \n",
    "    mean_return(env, new_pi)))\n",
    "print()\n",
    "print_state_value_function(new_V, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slippery Walk Five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SlipperyWalkFive-v0')\n",
    "P = env.env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    V = np.zeros(len(P), dtype=np.float64)\n",
    "    while True:\n",
    "        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "        for s in range(len(P)):\n",
    "            for a in range(len(P[s])):\n",
    "                for prob, next_state, reward, done in P[s][a]:\n",
    "                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n",
    "            break\n",
    "        V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "|           | 01      > | 02      > | 03      > | 04      > | 05      > |           |\n",
      "Reaches goal 95.10%. Obtains an average undiscounted return of 0.9510.\n",
      "\n",
      "State-value function:\n",
      "|           | 01   0.67 | 02   0.89 | 03   0.96 | 04   0.99 | 05   1.00 |           |\n"
     ]
    }
   ],
   "source": [
    "optimal_V, optimal_pi = value_iteration(P)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(optimal_pi, P, action_symbols=('<', '>'), n_cols=7)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, optimal_pi, goal_state=6)*100, \n",
    "    mean_return(env, optimal_pi)))\n",
    "print()\n",
    "print_state_value_function(optimal_V, P, n_cols=7)\n",
    "# |            | 01   0.668 | 02   0.890 | 03   0.964 | 04   0.989 | 05   0.997 |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "P = env.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (VI):\n",
      "Policy:\n",
      "| 00      < | 01      ^ | 02      ^ | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 73.20%. Obtains an average undiscounted return of 0.7320.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.54 | 01   0.50 | 02   0.47 | 03   0.46 |\n",
      "| 04   0.56 |           | 06   0.36 |           |\n",
      "| 08   0.59 | 09   0.64 | 10   0.62 |           |\n",
      "|           | 13   0.74 | 14   0.86 |           |\n"
     ]
    }
   ],
   "source": [
    "V_best_v, pi_best_v = value_iteration(P, gamma=0.99)\n",
    "print('Optimal policy and state-value function (VI):')\n",
    "print_policy(pi_best_v, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best_v, goal_state=15)*100, \n",
    "    mean_return(env, pi_best_v)))\n",
    "print()\n",
    "print_state_value_function(V_best_v, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For comparison, optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "| 00      < | 01      ^ | 02      ^ | 03      ^ |\n",
      "| 04      < |           | 06      < |           |\n",
      "| 08      ^ | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      v |           |\n",
      "Reaches goal 73.20%. Obtains an average undiscounted return of 0.7320.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.54 | 01   0.50 | 02   0.47 | 03   0.46 |\n",
      "| 04   0.56 |           | 06   0.36 |           |\n",
      "| 08   0.59 | 09   0.64 | 10   0.62 |           |\n",
      "|           | 13   0.74 | 14   0.86 |           |\n"
     ]
    }
   ],
   "source": [
    "print('For comparison, optimal policy and state-value function (PI):')\n",
    "print_policy(pi_best_p, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best_p, goal_state=15)*100, \n",
    "    mean_return(env, pi_best_p)))\n",
    "print()\n",
    "print_state_value_function(V_best_p, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the Frozen Lake environment MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "P = env.env.P\n",
    "\n",
    "# change reward function\n",
    "reward_goal, reward_holes, reward_others = 1, -1, -0.01\n",
    "goal, hole = 15, [5, 7, 11, 12]\n",
    "for s in range(len(P)):\n",
    "    for a in range(len(P[s])):\n",
    "        for t in range(len(P[s][a])):\n",
    "            values = list(P[s][a][t])\n",
    "            if values[1] == goal:\n",
    "                values[2] = reward_goal\n",
    "                values[3] = False\n",
    "            elif values[1] in hole:\n",
    "                values[2] = reward_holes\n",
    "                values[3] = False\n",
    "            else:\n",
    "                values[2] = reward_others\n",
    "                values[3] = False\n",
    "            if s in hole or s == goal:\n",
    "                values[2] = 0\n",
    "                values[3] = True\n",
    "            P[s][a][t] = tuple(values)\n",
    "\n",
    "# change transition function\n",
    "prob_action, prob_drift_one, prob_drift_two = 0.8, 0.1, 0.1\n",
    "for s in range(len(P)):\n",
    "    for a in range(len(P[s])):\n",
    "        for t in range(len(P[s][a])):\n",
    "            if P[s][a][t][0] == 1.0:\n",
    "                continue\n",
    "            values = list(P[s][a][t])\n",
    "            if t == 0:\n",
    "                values[0] = prob_drift_one\n",
    "            elif t == 1:\n",
    "                values[0] = prob_action\n",
    "            elif t == 2:\n",
    "                values[0] = prob_drift_two\n",
    "            P[s][a][t] = tuple(values)\n",
    "\n",
    "env.env.P = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "| 00      v | 01      ^ | 02      v | 03      ^ |\n",
      "| 04      < |           | 06      v |           |\n",
      "| 08      > | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      > |           |\n",
      "Reaches goal 85.20%. Obtains an average undiscounted return of 0.5281.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.43 | 01   0.35 | 02   0.41 | 03   0.28 |\n",
      "| 04   0.46 |           | 06   0.45 |           |\n",
      "| 08   0.64 | 09   0.88 | 10   0.83 |           |\n",
      "|           | 13   0.94 | 14   0.98 |           |\n"
     ]
    }
   ],
   "source": [
    "V_best, pi_best = policy_iteration(env.env.P, gamma=0.99)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(pi_best, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best, goal_state=15)*100, \n",
    "    mean_return(env, pi_best)))\n",
    "print()\n",
    "print_state_value_function(V_best, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "| 00      v | 01      ^ | 02      v | 03      ^ |\n",
      "| 04      < |           | 06      v |           |\n",
      "| 08      > | 09      v | 10      < |           |\n",
      "|           | 13      > | 14      > |           |\n",
      "Reaches goal 85.20%. Obtains an average undiscounted return of 0.5281.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.43 | 01   0.35 | 02   0.41 | 03   0.28 |\n",
      "| 04   0.46 |           | 06   0.45 |           |\n",
      "| 08   0.64 | 09   0.88 | 10   0.83 |           |\n",
      "|           | 13   0.94 | 14   0.98 |           |\n"
     ]
    }
   ],
   "source": [
    "V_best, pi_best = value_iteration(env.env.P, gamma=0.99)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(pi_best, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best, goal_state=15)*100, \n",
    "    mean_return(env, pi_best)))\n",
    "print()\n",
    "print_state_value_function(V_best, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russell & Norvig's Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('RussellNorvigGridworld-v0')\n",
    "P = env.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "| 00      > | 01      > | 02      > |           |\n",
      "| 04      ^ |           | 06      ^ |           |\n",
      "| 08      ^ | 09      < | 10      < | 11      < |\n",
      "Reaches goal 98.80%. Obtains an average undiscounted return of 0.7041.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.81 | 01   0.87 | 02   0.92 |           |\n",
      "| 04   0.76 |           | 06   0.66 |           |\n",
      "| 08   0.71 | 09   0.66 | 10   0.61 | 11   0.39 |\n"
     ]
    }
   ],
   "source": [
    "V_best_p, pi_best = policy_iteration(P)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(pi_best, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best, goal_state=3)*100, \n",
    "    mean_return(env, pi_best)))\n",
    "print()\n",
    "print_state_value_function(V_best_p, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy and state-value function (PI):\n",
      "Policy:\n",
      "| 00      > | 01      > | 02      > |           |\n",
      "| 04      ^ |           | 06      ^ |           |\n",
      "| 08      ^ | 09      < | 10      < | 11      < |\n",
      "Reaches goal 98.80%. Obtains an average undiscounted return of 0.7041.\n",
      "\n",
      "State-value function:\n",
      "| 00   0.81 | 01   0.87 | 02   0.92 |           |\n",
      "| 04   0.76 |           | 06   0.66 |           |\n",
      "| 08   0.71 | 09   0.66 | 10   0.61 | 11   0.39 |\n"
     ]
    }
   ],
   "source": [
    "V_best_v, pi_best = value_iteration(P)\n",
    "print('Optimal policy and state-value function (PI):')\n",
    "print_policy(pi_best, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi_best, goal_state=3)*100, \n",
    "    mean_return(env, pi_best)))\n",
    "print()\n",
    "print_state_value_function(V_best_v, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-construct optimal policy:\n",
      "Policy:\n",
      "| 00      > | 01      > | 02      > |           |\n",
      "| 04      ^ |           | 06      ^ |           |\n",
      "| 08      ^ | 09      < | 10      < | 11      < |\n",
      "Reaches goal 98.80%. Obtains an average undiscounted return of 0.7041.\n"
     ]
    }
   ],
   "source": [
    "WEST, SOUTH, EAST, NORTH = range(4)\n",
    "pi = lambda s: {\n",
    "    0:EAST, 1:EAST, 2:EAST, 3:WEST,\n",
    "    4:NORTH, 5:WEST, 6:NORTH, 7:WEST,\n",
    "    8:NORTH, 9:WEST, 10:WEST, 11:WEST\n",
    "}[s]\n",
    "print('Re-construct optimal policy:')\n",
    "print_policy(pi, P)\n",
    "print('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n",
    "    probability_success(env, pi, goal_state=3)*100, \n",
    "    mean_return(env, pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate optimal policy:\n",
      "State-value function:\n",
      "| 00   0.81 | 01   0.87 | 02   0.92 |           |\n",
      "| 04   0.76 |           | 06   0.66 |           |\n",
      "| 08   0.71 | 09   0.66 | 10   0.61 | 11   0.39 |\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(pi, P)\n",
    "print('Evaluate optimal policy:')\n",
    "print_state_value_function(V, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improve optimal policy (nothing to improve -- is the same policy, because it is optimal):\n",
      "Policy:\n",
      "| 00      > | 01      > | 02      > |           |\n",
      "| 04      ^ |           | 06      ^ |           |\n",
      "| 08      ^ | 09      < | 10      < | 11      < |\n"
     ]
    }
   ],
   "source": [
    "pi = policy_improvement(V, P)\n",
    "print('Improve optimal policy (nothing to improve -- is the same policy, because it is optimal):')\n",
    "print_policy(pi, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no differences, nothing to improve on the optimal policy and state-value function:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('There are no differences, nothing to improve on the optimal policy and state-value function:')\n",
    "print(np.abs(V_best_p - V))\n",
    "print(np.abs(V_best_v - V))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
