{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPG, TRPO+GAE, PPO, ACER, ACKTR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_settings = {\n",
    "    'training_env_name': 'Pendulum-v0',\n",
    "    'testing_env_name': 'Pendulum-v0',\n",
    "    'gamma': 0.99,\n",
    "    'max_episodes': 1000,\n",
    "    'max_mean_reward': -150,\n",
    "    'print_every_episodes': 25,\n",
    "    'save_every_episodes': 1500,\n",
    "    'n_evaluation_episodes': 10,\n",
    "}\n",
    "\n",
    "buffer = lambda: ReplayBuffer(max_samples=1000000, batch_size=100)\n",
    "value_model = lambda nS: FCV(nS, hidden_dims=(512,256))\n",
    "value_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "value_optimizer_lr = 0.01\n",
    "\n",
    "qvalue_model = lambda nS, nA: FCQV(nS, nA, hidden_dims=(512,256))\n",
    "qvalue_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "qvalue_optimizer_lr = 0.01\n",
    "\n",
    "policy_model = lambda nS, nA, max_action: FCSP(nS, nA, max_action, hidden_dims=(512,256))\n",
    "policy_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "policy_optimizer_lr = 0.001\n",
    "\n",
    "seed = 12345\n",
    "update_target_every_timesteps = 1\n",
    "train_every_timesteps = 1\n",
    "tau = 0.005\n",
    "max_gradient = float('inf')\n",
    "n_warmup_batch = 5\n",
    "\n",
    "training_env_name, testing_env_name, gamma, max_episodes, \\\n",
    "  max_mean_reward, print_every_episodes, save_every_episodes, \\\n",
    "  n_evaluation_episodes = environment_settings.values()\n",
    "\n",
    "strategy = lambda nA: OUDecayingNoise(\n",
    "    mu=np.zeros(nA), sigma=0.2 * np.ones(nA), \n",
    "    theta=0.15, max_steps=max_episodes*0.8)\n",
    "strategy = lambda nA: TD3Noise(nA)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "agent = SACAgent(buffer, value_model, value_optimizer, value_optimizer_lr,\n",
    "                 qvalue_model, qvalue_optimizer, qvalue_optimizer_lr,\n",
    "                 policy_model, policy_optimizer, policy_optimizer_lr, \n",
    "                 strategy, update_target_every_timesteps, \n",
    "                 train_every_timesteps, tau, max_gradient, n_warmup_batch, \n",
    "                 print_every_episodes, save_every_episodes=5000,\n",
    "                 experiment_name='default')\n",
    "\n",
    "training_env = make_monitored_env(\n",
    "    training_env_name, 'training', seed)\n",
    "rewards = agent.train(\n",
    "    training_env, gamma=gamma, \n",
    "    max_episodes=max_episodes, max_mean_reward=max_mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=training_env.videos, title='Training evolution', max_n_videos=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRPO + GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_settings = {\n",
    "    'training_env_name': 'Pendulum-v0',\n",
    "    'testing_env_name': 'Pendulum-v0',\n",
    "    'gamma': 0.99,\n",
    "    'max_episodes': 1000,\n",
    "    'max_mean_reward': -150,\n",
    "    'print_every_episodes': 25,\n",
    "    'save_every_episodes': 1500,\n",
    "    'n_evaluation_episodes': 10,\n",
    "}\n",
    "\n",
    "buffer = lambda: ReplayBuffer(max_samples=1000000, batch_size=100)\n",
    "value_model = lambda nS: FCV(nS, hidden_dims=(512,256))\n",
    "value_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "value_optimizer_lr = 0.01\n",
    "\n",
    "qvalue_model = lambda nS, nA: FCQV(nS, nA, hidden_dims=(512,256))\n",
    "qvalue_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "qvalue_optimizer_lr = 0.01\n",
    "\n",
    "policy_model = lambda nS, nA, max_action: FCSP(nS, nA, max_action, hidden_dims=(512,256))\n",
    "policy_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "policy_optimizer_lr = 0.001\n",
    "\n",
    "seed = 12345\n",
    "update_target_every_timesteps = 1\n",
    "train_every_timesteps = 1\n",
    "tau = 0.005\n",
    "max_gradient = float('inf')\n",
    "n_warmup_batch = 5\n",
    "\n",
    "training_env_name, testing_env_name, gamma, max_episodes, \\\n",
    "  max_mean_reward, print_every_episodes, save_every_episodes, \\\n",
    "  n_evaluation_episodes = environment_settings.values()\n",
    "\n",
    "strategy = lambda nA: OUDecayingNoise(\n",
    "    mu=np.zeros(nA), sigma=0.2 * np.ones(nA), \n",
    "    theta=0.15, max_steps=max_episodes*0.8)\n",
    "strategy = lambda nA: TD3Noise(nA)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "agent = SACAgent(buffer, value_model, value_optimizer, value_optimizer_lr,\n",
    "                 qvalue_model, qvalue_optimizer, qvalue_optimizer_lr,\n",
    "                 policy_model, policy_optimizer, policy_optimizer_lr, \n",
    "                 strategy, update_target_every_timesteps, \n",
    "                 train_every_timesteps, tau, max_gradient, n_warmup_batch, \n",
    "                 print_every_episodes, save_every_episodes=5000,\n",
    "                 experiment_name='default')\n",
    "\n",
    "training_env = make_monitored_env(\n",
    "    training_env_name, 'training', seed)\n",
    "rewards = agent.train(\n",
    "    training_env, gamma=gamma, \n",
    "    max_episodes=max_episodes, max_mean_reward=max_mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=training_env.videos, title='Training evolution', max_n_videos=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO+GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_settings = {\n",
    "    'training_env_name': 'Pendulum-v0',\n",
    "    'testing_env_name': 'Pendulum-v0',\n",
    "    'gamma': 0.99,\n",
    "    'max_episodes': 1000,\n",
    "    'max_mean_reward': -150,\n",
    "    'print_every_episodes': 25,\n",
    "    'save_every_episodes': 1500,\n",
    "    'n_evaluation_episodes': 10,\n",
    "}\n",
    "\n",
    "buffer = lambda: ReplayBuffer(max_samples=1000000, batch_size=100)\n",
    "value_model = lambda nS: FCV(nS, hidden_dims=(512,256))\n",
    "value_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "value_optimizer_lr = 0.01\n",
    "\n",
    "qvalue_model = lambda nS, nA: FCQV(nS, nA, hidden_dims=(512,256))\n",
    "qvalue_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "qvalue_optimizer_lr = 0.01\n",
    "\n",
    "policy_model = lambda nS, nA, max_action: FCSP(nS, nA, max_action, hidden_dims=(512,256))\n",
    "policy_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "policy_optimizer_lr = 0.001\n",
    "\n",
    "seed = 12345\n",
    "update_target_every_timesteps = 1\n",
    "train_every_timesteps = 1\n",
    "tau = 0.005\n",
    "max_gradient = float('inf')\n",
    "n_warmup_batch = 5\n",
    "\n",
    "training_env_name, testing_env_name, gamma, max_episodes, \\\n",
    "  max_mean_reward, print_every_episodes, save_every_episodes, \\\n",
    "  n_evaluation_episodes = environment_settings.values()\n",
    "\n",
    "strategy = lambda nA: OUDecayingNoise(\n",
    "    mu=np.zeros(nA), sigma=0.2 * np.ones(nA), \n",
    "    theta=0.15, max_steps=max_episodes*0.8)\n",
    "strategy = lambda nA: TD3Noise(nA)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "agent = SACAgent(buffer, value_model, value_optimizer, value_optimizer_lr,\n",
    "                 qvalue_model, qvalue_optimizer, qvalue_optimizer_lr,\n",
    "                 policy_model, policy_optimizer, policy_optimizer_lr, \n",
    "                 strategy, update_target_every_timesteps, \n",
    "                 train_every_timesteps, tau, max_gradient, n_warmup_batch, \n",
    "                 print_every_episodes, save_every_episodes=5000,\n",
    "                 experiment_name='default')\n",
    "\n",
    "training_env = make_monitored_env(\n",
    "    training_env_name, 'training', seed)\n",
    "rewards = agent.train(\n",
    "    training_env, gamma=gamma, \n",
    "    max_episodes=max_episodes, max_mean_reward=max_mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=training_env.videos, title='Training evolution', max_n_videos=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_settings = {\n",
    "    'training_env_name': 'Pendulum-v0',\n",
    "    'testing_env_name': 'Pendulum-v0',\n",
    "    'gamma': 0.99,\n",
    "    'max_episodes': 1000,\n",
    "    'max_mean_reward': -150,\n",
    "    'print_every_episodes': 25,\n",
    "    'save_every_episodes': 1500,\n",
    "    'n_evaluation_episodes': 10,\n",
    "}\n",
    "\n",
    "buffer = lambda: ReplayBuffer(max_samples=1000000, batch_size=100)\n",
    "value_model = lambda nS: FCV(nS, hidden_dims=(512,256))\n",
    "value_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "value_optimizer_lr = 0.01\n",
    "\n",
    "qvalue_model = lambda nS, nA: FCQV(nS, nA, hidden_dims=(512,256))\n",
    "qvalue_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "qvalue_optimizer_lr = 0.01\n",
    "\n",
    "policy_model = lambda nS, nA, max_action: FCSP(nS, nA, max_action, hidden_dims=(512,256))\n",
    "policy_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "policy_optimizer_lr = 0.001\n",
    "\n",
    "seed = 12345\n",
    "update_target_every_timesteps = 1\n",
    "train_every_timesteps = 1\n",
    "tau = 0.005\n",
    "max_gradient = float('inf')\n",
    "n_warmup_batch = 5\n",
    "\n",
    "training_env_name, testing_env_name, gamma, max_episodes, \\\n",
    "  max_mean_reward, print_every_episodes, save_every_episodes, \\\n",
    "  n_evaluation_episodes = environment_settings.values()\n",
    "\n",
    "strategy = lambda nA: OUDecayingNoise(\n",
    "    mu=np.zeros(nA), sigma=0.2 * np.ones(nA), \n",
    "    theta=0.15, max_steps=max_episodes*0.8)\n",
    "strategy = lambda nA: TD3Noise(nA)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "agent = SACAgent(buffer, value_model, value_optimizer, value_optimizer_lr,\n",
    "                 qvalue_model, qvalue_optimizer, qvalue_optimizer_lr,\n",
    "                 policy_model, policy_optimizer, policy_optimizer_lr, \n",
    "                 strategy, update_target_every_timesteps, \n",
    "                 train_every_timesteps, tau, max_gradient, n_warmup_batch, \n",
    "                 print_every_episodes, save_every_episodes=5000,\n",
    "                 experiment_name='default')\n",
    "\n",
    "training_env = make_monitored_env(\n",
    "    training_env_name, 'training', seed)\n",
    "rewards = agent.train(\n",
    "    training_env, gamma=gamma, \n",
    "    max_episodes=max_episodes, max_mean_reward=max_mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=training_env.videos, title='Training evolution', max_n_videos=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACKTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_settings = {\n",
    "    'training_env_name': 'Pendulum-v0',\n",
    "    'testing_env_name': 'Pendulum-v0',\n",
    "    'gamma': 0.99,\n",
    "    'max_episodes': 1000,\n",
    "    'max_mean_reward': -150,\n",
    "    'print_every_episodes': 25,\n",
    "    'save_every_episodes': 1500,\n",
    "    'n_evaluation_episodes': 10,\n",
    "}\n",
    "\n",
    "buffer = lambda: ReplayBuffer(max_samples=1000000, batch_size=100)\n",
    "value_model = lambda nS: FCV(nS, hidden_dims=(512,256))\n",
    "value_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "value_optimizer_lr = 0.01\n",
    "\n",
    "qvalue_model = lambda nS, nA: FCQV(nS, nA, hidden_dims=(512,256))\n",
    "qvalue_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "qvalue_optimizer_lr = 0.01\n",
    "\n",
    "policy_model = lambda nS, nA, max_action: FCSP(nS, nA, max_action, hidden_dims=(512,256))\n",
    "policy_optimizer = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "policy_optimizer_lr = 0.001\n",
    "\n",
    "seed = 12345\n",
    "update_target_every_timesteps = 1\n",
    "train_every_timesteps = 1\n",
    "tau = 0.005\n",
    "max_gradient = float('inf')\n",
    "n_warmup_batch = 5\n",
    "\n",
    "training_env_name, testing_env_name, gamma, max_episodes, \\\n",
    "  max_mean_reward, print_every_episodes, save_every_episodes, \\\n",
    "  n_evaluation_episodes = environment_settings.values()\n",
    "\n",
    "strategy = lambda nA: OUDecayingNoise(\n",
    "    mu=np.zeros(nA), sigma=0.2 * np.ones(nA), \n",
    "    theta=0.15, max_steps=max_episodes*0.8)\n",
    "strategy = lambda nA: TD3Noise(nA)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "agent = SACAgent(buffer, value_model, value_optimizer, value_optimizer_lr,\n",
    "                 qvalue_model, qvalue_optimizer, qvalue_optimizer_lr,\n",
    "                 policy_model, policy_optimizer, policy_optimizer_lr, \n",
    "                 strategy, update_target_every_timesteps, \n",
    "                 train_every_timesteps, tau, max_gradient, n_warmup_batch, \n",
    "                 print_every_episodes, save_every_episodes=5000,\n",
    "                 experiment_name='default')\n",
    "\n",
    "training_env = make_monitored_env(\n",
    "    training_env_name, 'training', seed)\n",
    "rewards = agent.train(\n",
    "    training_env, gamma=gamma, \n",
    "    max_episodes=max_episodes, max_mean_reward=max_mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(data=get_gif_html(env_videos=training_env.videos, title='Training evolution', max_n_videos=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
